@article{yamoah_deep_2022,
	title = {Deepfake Speech and Automatic Speaker Verification Systems in the {African} Setting},
	url = {https://hdl.handle.net/20.500.11988/951},
	abstract = {Automatic Speaker Verification (ASV) systems authenticate anyone who interacts with a digital system using speech in a seamless manner. ASV systems are widely utilised in the banking industry to validate customer identities. Deep neural network (DNN)- based voice synthesis systems, on the other hand, have enabled the cloning of the human voice in the form of deepfake audio, which can fool most humans and ASV systems. If these deepfake audios are used maliciously, they jeopardise people's identities and impede the security of ASV systems. This study documents efforts and findings from an exhaustive experimental study on the impact of deepfake audio generated on African accents on ASV systems. We found that deepfake audio generated on African accents is less likely to fool modern ASV systems. These findings highlight the importance of deepfake audio systems that can simulate convincing African accents to ensure that current technologies are used to tackle modern challenges in Africa.},
	language = {en},
	urldate = {2023-10-24},
	author = {Yamoah, Kweku Andoh and Fuseini, Hussein},
	month = may,
	year = {2022},
}

@incollection{CONRAD2017117,
        title = {Chapter 5 - Domain 5: Identity and access management (controlling access and managing identity)},
        editor = {Eric Conrad and Seth Misenar and Joshua Feldman},
        booktitle = {Eleventh Hour CISSP® (Third Edition)},
        publisher = {Syngress},
        edition = {Third Edition},
        pages = {117-134},
        year = {2017},
        isbn = {978-0-12-811248-9},
        doi = {https://doi.org/10.1016/B978-0-12-811248-9.00005-X},
        url = {https://www.sciencedirect.com/science/article/pii/B978012811248900005X},
        author = {Eric Conrad and Seth Misenar and Joshua Feldman},
        keywords = {Crossover error rate, Discretionary access control, False accept rate, False reject rate, Mandatory access control, Role-based access controls},
        abstract = {This chapter focuses on the application of access control models such as mandatory access control (MAC), discretionary access control (DAC), and role-based access control (RBAC) represent a significant amount of this domain’s material. Understanding the key categories of access control defenses, as well as preventive, detective, corrective, recovery, deterrent, and compensating controls, is necessary for this and numerous other domains. The final major content area in this chapter is dealing with authentication by introducing methods, protocols, and concepts related to ensuring that an identity claim can be validated appropriately.}
}
@article{kietzmann_deepfakes:_2020,
	title = {Deepfakes: {Trick} or treat?},
	volume = {63},
	issn = {00076813},
	shorttitle = {Deepfakes},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0007681319301600},
	doi = {10.1016/j.bushor.2019.11.006},
	language = {en},
	number = {2},
	urldate = {2021-09-02},
	journal = {Business Horizons},
	author = {Kietzmann, Jan and Lee, Linda W. and McCarthy, Ian P. and Kietzmann, Tim C.},
	month = mar,
	year = {2020},
	pages = {135--146},
}

@article{fried_text-based_2019,
	title = {Text-based editing of talking-head video},
	volume = {38},
	issn = {0730-0301, 1557-7368},
	url = {https://dl.acm.org/doi/10.1145/3306346.3323028},
	doi = {10.1145/3306346.3323028},
	language = {en},
	number = {4},
	urldate = {2021-09-02},
	journal = {ACM Transactions on Graphics},
	author = {Fried, Ohad and Tewari, Ayush and Zollhöfer, Michael and Finkelstein, Adam and Shechtman, Eli and Goldman, Dan B and Genova, Kyle and Jin, Zeyu and Theobalt, Christian and Agrawala, Maneesh},
	month = jul,
	year = {2019},
	pages = {1--14},
}

@misc{noauthor_deepfakes_nodate,
	title = {Deepfakes, explained},
	url = {https://mitsloan.mit.edu/ideas-made-to-matter/deepfakes-explained},
	abstract = {What are deepfakes, and how business leaders can learn to identify and protect against them.},
	language = {en},
	urldate = {2021-09-02},
	journal = {MIT Sloan},
}

@misc{david_council_nodate,
	title = {Council post: analyzing the rise of deepfake voice technology},
	shorttitle = {Council post},
	url = {https://www.forbes.com/sites/forbestechcouncil/2021/05/10/analyzing-the-rise-of-deepfake-voice-technology/},
	abstract = {Today, businesses are at risk of deepfake attacks, and the potential impact on their businesses could be substantial.},
	language = {en},
	urldate = {2021-09-02},
	journal = {Forbes},
	author = {David, Dominic},
}

@article{lavan_listeners_2019,
	title = {Listeners form average-based representations of individual voice identities},
	volume = {10},
	copyright = {2019 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-019-10295-w},
	doi = {10.1038/s41467-019-10295-w},
	abstract = {Models of voice perception propose that identities are encoded relative to an abstracted average or prototype. While there is some evidence for norm-based coding when learning to discriminate different voices, little is known about how the representation of an individual's voice identity is formed through variable exposure to that voice. In two experiments, we show evidence that participants form abstracted representations of individual voice identities based on averages, despite having never been exposed to these averages during learning. We created 3 perceptually distinct voice identities, fully controlling their within-person variability. Listeners first learned to recognise these identities based on ring-shaped distributions located around the perimeter of within-person voice spaces – crucially, these distributions were missing their centres. At test, listeners’ accuracy for old/new judgements was higher for stimuli located on an untrained distribution nested around the centre of each ring-shaped distribution compared to stimuli on the trained ring-shaped distribution.},
	language = {en},
	number = {1},
	urldate = {2021-09-03},
	journal = {Nature Communications},
	author = {Lavan, Nadine and Knight, Sarah and McGettigan, Carolyn},
	month = jun,
	year = {2019},
	pages = {2404},
}

@book{jurafsky__speech_nodate,
	edition = {3rd Edition Draft},
	title = {Speech and language processing},
	url = {https://web.stanford.edu/~jurafsky/slp3/},
	abstract = {Speech and Language Processing},
	urldate = {2021-09-06},
	author = {Jurafsky , Dan   and Martin, James  H  },
}

@article{partila_deep_2020,
	title = {Deep learning serves voice cloning: how vulnerable are automatic speaker veriﬁcation systems to spooﬁng trials?},
	volume = {58},
	issn = {0163-6804, 1558-1896},
	shorttitle = {Deep learning serves voice cloning},
	url = {https://ieeexplore.ieee.org/document/8999436/},
	doi = {10.1109/MCOM.001.1900396},
	number = {2},
	urldate = {2021-09-06},
	journal = {IEEE Communications Magazine},
	author = {Partila, Pavol and Tovarek, Jaromir and Ilk, Gokhan Hakki and Rozhon, Jan and Voznak, Miroslav},
	month = feb,
	year = {2020},
	pages = {100--105},
}

@inproceedings{malilk_fighting_2019,
	address = {Porto, Portugal},
	title = {Fighting ai with ai: fake speech detection using deep learning},
	url = {https://par.nsf.gov/servlets/purl/10109075},
	language = {English},
	booktitle = {Audio {Engineering} {Society}},
	publisher = {Journal of the Audio Engineering Society},
	author = {Malilk, Hafiz and Changalvala, Raghavendar},
	month = jun,
	year = {2019},
}

@article{arik_neural_2018,
	title = {Neural voice cloning with a few samples},
	url = {http://arxiv.org/abs/1802.06006},
	abstract = {Voice cloning is a highly desired feature for personalized speech interfaces. Neural network based speech synthesis has been shown to generate high quality speech for a large number of speakers. In this paper, we introduce a neural voice cloning system that takes a few audio samples as input. We study two approaches: speaker adaptation and speaker encoding. Speaker adaptation is based on fine-tuning a multi-speaker generative model with a few cloning samples. Speaker encoding is based on training a separate model to directly infer a new speaker embedding from cloning audios and to be used with a multi-speaker generative model. In terms of naturalness of the speech and its similarity to original speaker, both approaches can achieve good performance, even with very few cloning audios. While speaker adaptation can achieve better naturalness and similarity, the cloning time or required memory for the speaker encoding approach is significantly less, making it favorable for low-resource deployment.},
	urldate = {2021-09-10},
	journal = {arXiv:1802.06006 [cs, eess]},
	author = {Arik, Sercan O. and Chen, Jitong and Peng, Kainan and Ping, Wei and Zhou, Yanqi},
	month = oct,
	year = {2018},
	note = {arXiv: 1802.06006},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@article{ping_deep_2018,
	title = {Deep voice 3: scaling text-to-speech with convolutional sequence learning},
	shorttitle = {Deep {Voice} 3},
	url = {http://arxiv.org/abs/1710.07654},
	abstract = {We present Deep Voice 3, a fully-convolutional attention-based neural text-to-speech (TTS) system. Deep Voice 3 matches state-of-the-art neural speech synthesis systems in naturalness while training ten times faster. We scale Deep Voice 3 to data set sizes unprecedented for TTS, training on more than eight hundred hours of audio from over two thousand speakers. In addition, we identify common error modes of attention-based speech synthesis networks, demonstrate how to mitigate them, and compare several different waveform synthesis methods. We also describe how to scale inference to ten million queries per day on one single-GPU server.},
	urldate = {2021-09-10},
	journal = {arXiv:1710.07654 [cs, eess]},
	author = {Ping, Wei and Peng, Kainan and Gibiansky, Andrew and Arik, Sercan O. and Kannan, Ajay and Narang, Sharan and Raiman, Jonathan and Miller, John},
	month = feb,
	year = {2018},
	note = {arXiv: 1710.07654},
	keywords = {Computer Science - Sound, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@inproceedings{goodfellow_generative_2014,
	title = {Generative adversarial nets},
	volume = {27},
	url = {https://papers.nips.cc/paper/2014/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html},
	urldate = {2021-09-12},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	year = {2014},
}

@article{oord_wavenet:_2016,
	title = {Wavenet: a generative model for raw audio},
	shorttitle = {Wavenet},
	url = {http://arxiv.org/abs/1609.03499},
	abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
	urldate = {2021-09-12},
	journal = {arXiv:1609.03499 [cs]},
	author = {Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.03499},
	keywords = {Computer Science - Sound, Computer Science - Machine Learning},
}

@article{arik_deep_2017,
	title = {Deep voice: real-time neural text-to-speech},
	shorttitle = {Deep voice},
	url = {http://arxiv.org/abs/1702.07825},
	abstract = {We present Deep Voice, a production-quality text-to-speech system constructed entirely from deep neural networks. Deep Voice lays the groundwork for truly end-to-end neural speech synthesis. The system comprises five major building blocks: a segmentation model for locating phoneme boundaries, a grapheme-to-phoneme conversion model, a phoneme duration prediction model, a fundamental frequency prediction model, and an audio synthesis model. For the segmentation model, we propose a novel way of performing phoneme boundary detection with deep neural networks using connectionist temporal classification (CTC) loss. For the audio synthesis model, we implement a variant of WaveNet that requires fewer parameters and trains faster than the original. By using a neural network for each component, our system is simpler and more flexible than traditional text-to-speech systems, where each component requires laborious feature engineering and extensive domain expertise. Finally, we show that inference with our system can be performed faster than real time and describe optimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x speedups over existing implementations.},
	urldate = {2021-09-12},
	journal = {arXiv:1702.07825 [cs]},
	author = {Arik, Sercan O. and Chrzanowski, Mike and Coates, Adam and Diamos, Gregory and Gibiansky, Andrew and Kang, Yongguo and Li, Xian and Miller, John and Ng, Andrew and Raiman, Jonathan and Sengupta, Shubho and Shoeybi, Mohammad},
	month = mar,
	year = {2017},
	note = {arXiv: 1702.07825},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Sound},
}

@article{lorenzo-trueba_can_2018,
	title = {Can we steal your vocal identity from the {Internet}?: {Initial} investigation of cloning {Obama}'s voice using {GAN}, {WaveNet} and low-quality found data},
	shorttitle = {Can we steal your vocal identity from the {Internet}?},
	url = {http://arxiv.org/abs/1803.00860},
	abstract = {Thanks to the growing availability of spoofing databases and rapid advances in using them, systems for detecting voice spoofing attacks are becoming more and more capable, and error rates close to zero are being reached for the ASVspoof2015 database. However, speech synthesis and voice conversion paradigms that are not considered in the ASVspoof2015 database are appearing. Such examples include direct waveform modelling and generative adversarial networks. We also need to investigate the feasibility of training spoofing systems using only low-quality found data. For that purpose, we developed a generative adversarial network-based speech enhancement system that improves the quality of speech data found in publicly available sources. Using the enhanced data, we trained state-of-the-art text-to-speech and voice conversion models and evaluated them in terms of perceptual speech quality and speaker similarity. The results show that the enhancement models significantly improved the SNR of low-quality degraded data found in publicly available sources and that they significantly improved the perceptual cleanliness of the source speech without significantly degrading the naturalness of the voice. However, the results also show limitations when generating speech with the low-quality found data.},
	urldate = {2021-09-13},
	journal = {arXiv:1803.00860 [cs, eess, stat]},
	author = {Lorenzo-Trueba, Jaime and Fang, Fuming and Wang, Xin and Echizen, Isao and Yamagishi, Junichi and Kinnunen, Tomi},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.00860},
	keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Computation and Language, Computer Science - Sound, Statistics - Machine Learning},
}

@article{suwajanakorn_synthesizing_2017,
	title = {Synthesizing {Obama}: learning lip sync from audio},
	volume = {36},
	issn = {0730-0301, 1557-7368},
	shorttitle = {Synthesizing obama},
	url = {https://dl.acm.org/doi/10.1145/3072959.3073640},
	doi = {10.1145/3072959.3073640},
	language = {en},
	number = {4},
	urldate = {2021-09-17},
	journal = {ACM Transactions on Graphics},
	author = {Suwajanakorn, Supasorn and Seitz, Steven M. and Kemelmacher-Shlizerman, Ira},
	month = jul,
	year = {2017},
	pages = {1--13},
}

@book{dineshraj_gunasekaran_improved_2020,
	title = {Improved speech synthesis using generative adversarial networks},
	publisher = {ACIS},
	author = {{Dineshraj Gunasekaran} and {Gautham Venkatraj} and {Eoin Brophy} and {Tomas Ward}},
	year = {2020},
}

@article{nguyen_deep_2021,
	title = {Deep learning for deepfakes creation and detection: a survey},
	shorttitle = {Deep learning for deepfakes creation and detection},
	url = {http://arxiv.org/abs/1909.11573},
	abstract = {Deep learning has been successfully applied to solve various complex problems ranging from big data analytics to computer vision and human-level control. Deep learning advances however have also been employed to create software that can cause threats to privacy, democracy and national security. One of those deep learning-powered applications recently emerged is deepfake. Deepfake algorithms can create fake images and videos that humans cannot distinguish them from authentic ones. The proposal of technologies that can automatically detect and assess the integrity of digital visual media is therefore indispensable. This paper presents a survey of algorithms used to create deepfakes and, more importantly, methods proposed to detect deepfakes in the literature to date. We present extensive discussions on challenges, research trends and directions related to deepfake technologies. By reviewing the background of deepfakes and state-of-the-art deepfake detection methods, this study provides a comprehensive overview of deepfake techniques and facilitates the development of new and more robust methods to deal with the increasingly challenging deepfakes.},
	urldate = {2021-09-26},
	journal = {arXiv:1909.11573 [cs, eess]},
	author = {Nguyen, Thanh Thi and Nguyen, Quoc Viet Hung and Nguyen, Cuong M. and Nguyen, Dung and Nguyen, Duc Thanh and Nahavandi, Saeid},
	month = apr,
	year = {2021},
	note = {arXiv: 1909.11573},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
}

@misc{noauthor__nodate,
	title = { {FakeApp} 2.2.0 - {Download} for {PC} {Free}. {Malavida}. {Retrieved} {September} 26, 2021 from https://www.malavida.com/en/soft/fakeapp/},
	url = {https://www.malavida.com/en/soft/fakeapp/},
	abstract = {8/10 (606 votes) - Download FakeApp Free. FakeApp is a program that lets you create masks capable of swapping faces on videos by means of the artificial intelligence developed by a Reddit user. Mobile face swap applications have already been around for quite a few years. Some people consider them...},
	language = {en},
	urldate = {2021-09-26},
	journal = {Malavida},
}

@misc{noauthor_faceswap:deepfakes_nodate,
	title = {Faceswap:{Deepfakes} software for all.{Retrieved} 26-{Sep}-21 from https://github.com/deepfakes/faceswap},
	url = {https://github.com/deepfakes/faceswap},
}

@article{karras_style-based_2019,
	title = {A style-based generator architecture for generative adversarial networks},
	url = {http://arxiv.org/abs/1812.04948},
	abstract = {We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.},
	urldate = {2021-09-27},
	journal = {arXiv:1812.04948 [cs, stat]},
	author = {Karras, Tero and Laine, Samuli and Aila, Timo},
	month = mar,
	year = {2019},
	note = {arXiv: 1812.04948},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{zhu_unpaired_2020,
	title = {Unpaired image-to-image translation using cycle-consistent adversarial networks},
	url = {http://arxiv.org/abs/1703.10593},
	abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain \$X\$ to a target domain \$Y\$ in the absence of paired examples. Our goal is to learn a mapping \$G: X {\textbackslash}rightarrow Y\$ such that the distribution of images from \$G(X)\$ is indistinguishable from the distribution \$Y\$ using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping \$F: Y {\textbackslash}rightarrow X\$ and introduce a cycle consistency loss to push \$F(G(X)) {\textbackslash}approx X\$ (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
	urldate = {2021-09-27},
	journal = {arXiv:1703.10593 [cs]},
	author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
	month = aug,
	year = {2020},
	note = {arXiv: 1703.10593},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@incollection{dasgupta_overview_2021,
	address = {Cham},
	title = {Overview of gans for image synthesis and detection methods},
	isbn = {9783030556914 9783030556921},
	url = {http://link.springer.com/10.1007/978-3-030-55692-1_5},
	language = {en},
	urldate = {2021-09-27},
	booktitle = {Adversary-{Aware} {Learning} {Techniques} and {Trends} in {Cybersecurity}},
	publisher = {Springer International Publishing},
	author = {Tjon, Eric and Moh, Melody and Moh, Teng-Sheng},
	editor = {Dasgupta, Prithviraj and Collins, Joseph B. and Mittu, Ranjeev},
	year = {2021},
	doi = {10.1007/978-3-030-55692-1_5},
	pages = {85--101},
}

@article{wenger_hello_2021,
	title = {"{Hello}, it's me": deep learning-based speech synthesis attacks in the real world},
	shorttitle = {"{Hello}, it's me"},
	url = {http://arxiv.org/abs/2109.09598},
	doi = {10.1145/3460120.3484742},
	abstract = {Advances in deep learning have introduced a new wave of voice synthesis tools, capable of producing audio that sounds as if spoken by a target speaker. If successful, such tools in the wrong hands will enable a range of powerful attacks against both humans and software systems (aka machines). This paper documents efforts and findings from a comprehensive experimental study on the impact of deep-learning based speech synthesis attacks on both human listeners and machines such as speaker recognition and voice-signin systems. We find that both humans and machines can be reliably fooled by synthetic speech and that existing defenses against synthesized speech fall short. These findings highlight the need to raise awareness and develop new protections against synthetic speech for both humans and machines.},
	urldate = {2021-09-28},
	journal = {arXiv:2109.09598 [cs, eess]},
	author = {Wenger, Emily and Bronckers, Max and Cianfarani, Christian and Cryan, Jenna and Sha, Angela and Zheng, Haitao and Zhao, Ben Y.},
	month = sep,
	year = {2021},
	note = {arXiv: 2109.09598},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Artificial Intelligence, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@article{muller_human_2021,
	title = {Human perception of audio deepfakes},
	url = {http://arxiv.org/abs/2107.09667},
	abstract = {The recent emergence of deepfakes, computerized realistic multimedia fakes, brought the detection of manipulated and generated content to the forefront. While many machine learning models for deepfakes detection have been proposed, the human detection capabilities have remained far less explored. This is of special importance as human perception differs from machine perception and deepfakes are generally designed to fool the human. So far, this issue has only been addressed in the area of images and video. To compare the ability of humans and machines in detecting audio deepfakes, we conducted an online gamified experiment in which we asked users to discern bonda-fide audio samples from spoofed audio, generated with a variety of algorithms. 200 users competed for 8976 game rounds with an artificial intelligence (AI) algorithm trained for audio deepfake detection. With the collected data we found that the machine generally outperforms the humans in detecting audio deepfakes, but that the converse holds for a certain attack type, for which humans are still more accurate. Furthermore, we found that younger participants are on average better at detecting audio deepfakes than older participants, while IT-professionals hold no advantage over laymen. We conclude that it is important to combine human and machine knowledge in order to improve audio deepfake detection.},
	urldate = {2021-09-28},
	journal = {arXiv:2107.09667 [cs, eess]},
	author = {Müller, Nicolas M. and Markert, Karla and Böttinger, Konstantin},
	month = jul,
	year = {2021},
	note = {arXiv: 2107.09667},
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Artificial Intelligence, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@article{jia_transfer_2019,
	title = {Transfer learning from speaker verification to multispeaker text-to-speech synthesis},
	url = {http://arxiv.org/abs/1806.04558},
	abstract = {We describe a neural network-based system for text-to-speech (TTS) synthesis that is able to generate speech audio in the voice of many different speakers, including those unseen during training. Our system consists of three independently trained components: (1) a speaker encoder network, trained on a speaker verification task using an independent dataset of noisy speech from thousands of speakers without transcripts, to generate a fixed-dimensional embedding vector from seconds of reference speech from a target speaker; (2) a sequence-to-sequence synthesis network based on Tacotron 2, which generates a mel spectrogram from text, conditioned on the speaker embedding; (3) an auto-regressive WaveNet-based vocoder that converts the mel spectrogram into a sequence of time domain waveform samples. We demonstrate that the proposed model is able to transfer the knowledge of speaker variability learned by the discriminatively-trained speaker encoder to the new task, and is able to synthesize natural speech from speakers that were not seen during training. We quantify the importance of training the speaker encoder on a large and diverse speaker set in order to obtain the best generalization performance. Finally, we show that randomly sampled speaker embeddings can be used to synthesize speech in the voice of novel speakers dissimilar from those used in training, indicating that the model has learned a high quality speaker representation.},
	urldate = {2021-10-01},
	journal = {arXiv:1806.04558 [cs, eess]},
	author = {Jia, Ye and Zhang, Yu and Weiss, Ron J. and Wang, Quan and Shen, Jonathan and Ren, Fei and Chen, Zhifeng and Nguyen, Patrick and Pang, Ruoming and Moreno, Ignacio Lopez and Wu, Yonghui},
	month = jan,
	year = {2019},
	note = {arXiv: 1806.04558},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@article{qian_autovc:_2019,
	title = {Autovc: zero-shot voice style transfer with only autoencoder loss},
	shorttitle = {Autovc},
	url = {http://arxiv.org/abs/1905.05879},
	abstract = {Non-parallel many-to-many voice conversion, as well as zero-shot voice conversion, remain under-explored areas. Deep style transfer algorithms, such as generative adversarial networks (GAN) and conditional variational autoencoder (CVAE), are being applied as new solutions in this field. However, GAN training is sophisticated and difficult, and there is no strong evidence that its generated speech is of good perceptual quality. On the other hand, CVAE training is simple but does not come with the distribution-matching property of a GAN. In this paper, we propose a new style transfer scheme that involves only an autoencoder with a carefully designed bottleneck. We formally show that this scheme can achieve distribution-matching style transfer by training only on a self-reconstruction loss. Based on this scheme, we proposed AUTOVC, which achieves state-of-the-art results in many-to-many voice conversion with non-parallel data, and which is the first to perform zero-shot voice conversion.},
	urldate = {2021-10-01},
	journal = {arXiv:1905.05879 [cs, eess, stat]},
	author = {Qian, Kaizhi and Zhang, Yang and Chang, Shiyu and Yang, Xuesong and Hasegawa-Johnson, Mark},
	month = jun,
	year = {2019},
	note = {arXiv: 1905.05879},
	keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Sound, Statistics - Machine Learning},
}

@article{kong_hifi-gan:_2020,
	title = {{HiFi}-{GAN}: {Generative} {Adversarial} {Networks} for {Efficient} and {High} {Fidelity} {Speech} {Synthesis}},
	shorttitle = {{HiFi}-{GAN}},
	url = {http://arxiv.org/abs/2010.05646},
	abstract = {Several recent work on speech synthesis have employed generative adversarial networks (GANs) to produce raw waveforms. Although such methods improve the sampling efficiency and memory usage, their sample quality has not yet reached that of autoregressive and flow-based generative models. In this work, we propose HiFi-GAN, which achieves both efficient and high-fidelity speech synthesis. As speech audio consists of sinusoidal signals with various periods, we demonstrate that modeling periodic patterns of an audio is crucial for enhancing sample quality. A subjective human evaluation (mean opinion score, MOS) of a single speaker dataset indicates that our proposed method demonstrates similarity to human quality while generating 22.05 kHz high-fidelity audio 167.9 times faster than real-time on a single V100 GPU. We further show the generality of HiFi-GAN to the mel-spectrogram inversion of unseen speakers and end-to-end speech synthesis. Finally, a small footprint version of HiFi-GAN generates samples 13.4 times faster than real-time on CPU with comparable quality to an autoregressive counterpart.},
	urldate = {2021-10-18},
	journal = {arXiv:2010.05646 [cs, eess]},
	author = {Kong, Jungil and Kim, Jaehyeon and Bae, Jaekyoung},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.05646},
	keywords = {Computer Science - Sound, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@article{kumar_melgan:_2019,
	title = {{MelGAN}: {Generative} {Adversarial} {Networks} for {Conditional} {Waveform} {Synthesis}},
	shorttitle = {{MelGAN}},
	url = {http://arxiv.org/abs/1910.06711},
	abstract = {Previous works (Donahue et al., 2018a; Engel et al., 2019a) have found that generating coherent raw audio waveforms with GANs is challenging. In this paper, we show that it is possible to train GANs reliably to generate high quality coherent waveforms by introducing a set of architectural changes and simple training techniques. Subjective evaluation metric (Mean Opinion Score, or MOS) shows the effectiveness of the proposed approach for high quality mel-spectrogram inversion. To establish the generality of the proposed techniques, we show qualitative results of our model in speech synthesis, music domain translation and unconditional music synthesis. We evaluate the various components of the model through ablation studies and suggest a set of guidelines to design general purpose discriminators and generators for conditional sequence synthesis tasks. Our model is non-autoregressive, fully convolutional, with significantly fewer parameters than competing models and generalizes to unseen speakers for mel-spectrogram inversion. Our pytorch implementation runs at more than 100x faster than realtime on GTX 1080Ti GPU and more than 2x faster than real-time on CPU, without any hardware specific optimization tricks.},
	urldate = {2021-10-18},
	journal = {arXiv:1910.06711 [cs, eess]},
	author = {Kumar, Kundan and Kumar, Rithesh and de Boissiere, Thibault and Gestin, Lucas and Teoh, Wei Zhen and Sotelo, Jose and de Brebisson, Alexandre and Bengio, Yoshua and Courville, Aaron},
	month = dec,
	year = {2019},
	note = {arXiv: 1910.06711},
	keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound},
}

@article{zhang_gazev:_2020,
	title = {{GAZEV}: {GAN}-{Based} {Zero}-{Shot} {Voice} {Conversion} over {Non}-parallel {Speech} {Corpus}},
	shorttitle = {{GAZEV}},
	url = {http://arxiv.org/abs/2010.12788},
	abstract = {Non-parallel many-to-many voice conversion is recently attract-ing huge research efforts in the speech processing community. A voice conversion system transforms an utterance of a source speaker to another utterance of a target speaker by keeping the content in the original utterance and replacing by the vocal features from the target speaker. Existing solutions, e.g., StarGAN-VC2, present promising results, only when speech corpus of the engaged speakers is available during model training. AUTOVCis able to perform voice conversion on unseen speakers, but it needs an external pretrained speaker verification model. In this paper, we present our new GAN-based zero-shot voice conversion solution, called GAZEV, which targets to support unseen speakers on both source and target utterances. Our key technical contribution is the adoption of speaker embedding loss on top of the GAN framework, as well as adaptive instance normalization strategy, in order to address the limitations of speaker identity transfer in existing solutions. Our empirical evaluations demonstrate significant performance improvement on output speech quality and comparable speaker similarity to AUTOVC.},
	urldate = {2021-10-18},
	journal = {arXiv:2010.12788 [cs, eess]},
	author = {Zhang, Zining and He, Bingsheng and Zhang, Zhenjie},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.12788},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@inproceedings{wan_generalized_2018,
	address = {Calgary, AB},
	title = {Generalized {End}-to-{End} {Loss} for {Speaker} {Verification}},
	isbn = {9781538646588},
	url = {https://ieeexplore.ieee.org/document/8462665/},
	doi = {10.1109/ICASSP.2018.8462665},
	urldate = {2021-10-18},
	booktitle = {2018 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Wan, Li and Wang, Quan and Papir, Alan and Moreno, Ignacio Lopez},
	month = apr,
	year = {2018},
	pages = {4879--4883},
}

@article{shen_natural_2018,
	title = {Natural {TTS} {Synthesis} by {Conditioning} {WaveNet} on {Mel} {Spectrogram} {Predictions}},
	url = {http://arxiv.org/abs/1712.05884},
	abstract = {This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize timedomain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of \$4.53\$ comparable to a MOS of \$4.58\$ for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the input to WaveNet instead of linguistic, duration, and \$F\_0\$ features. We further demonstrate that using a compact acoustic intermediate representation enables significant simplification of the WaveNet architecture.},
	urldate = {2021-10-18},
	journal = {arXiv:1712.05884 [cs]},
	author = {Shen, Jonathan and Pang, Ruoming and Weiss, Ron J. and Schuster, Mike and Jaitly, Navdeep and Yang, Zongheng and Chen, Zhifeng and Zhang, Yu and Wang, Yuxuan and Skerry-Ryan, R. J. and Saurous, Rif A. and Agiomyrgiannakis, Yannis and Wu, Yonghui},
	month = feb,
	year = {2018},
	note = {arXiv: 1712.05884},
	keywords = {Computer Science - Computation and Language},
}

@article{veaux_superseded_2017,
	title = {{SUPERSEDED} - {CSTR} {VCTK} {Corpus}: {English} {Multi}-speaker {Corpus} for {CSTR} {Voice} {Cloning} {Toolkit}},
	copyright = {Creative Commons Attribution 4.0 International Public License},
	shorttitle = {{SUPERSEDED} - {CSTR} {VCTK} {Corpus}},
	url = {https://datashare.ed.ac.uk/handle/10283/2651},
	doi = {10.7488/ds/1994},
	abstract = {\#\# This item has been replaced by the one which can be found at https://doi.org/10.7488/ds/2645 \#\#' This CSTR VCTK Corpus (Centre for Speech Technology Voice Cloning Toolkit) includes speech data uttered by 109 native speakers of English with various accents. 96kHz versions of the recordings are available at https://doi.org/10.7488/ds/2101. Each speaker reads out about 400 sentences, most of which were selected from a newspaper plus the Rainbow Passage and an elicitation paragraph intended to identify the speaker's accent. The newspaper texts were taken from The Herald (Glasgow), with permission from Herald \& Times Group. Each speaker reads a different set of the newspaper sentences, where each set was selected using a greedy algorithm designed to maximise the contextual and phonetic coverage. The Rainbow Passage and elicitation paragraph are the same for all speakers. The Rainbow Passage can be found in the International Dialects of English Archive: (http://web.ku.edu/{\textasciitilde}idea/readings/rainbow.htm). The elicitation paragraph is identical to the one used for the speech accent archive (http://accent.gmu.edu). The details of the speech accent archive can be found at http://www.ualberta.ca/{\textasciitilde}aacl2009/PDFs/WeinbergerKunath2009AACL.pdf . All speech data were recorded using an identical recording setup: an omni-directional head-mounted microphone (DPA 4035), 96kHz sampling frequency at 24 bits and in a hemi-anechoic chamber of the University of Edinburgh. All recordings were converted into 16 bits, downsampled to 48 kHz based on STPK, and manually end-pointed. This corpus was recorded for the purpose of building HMM-based text-to-speech synthesis systems, especially for speaker-adaptive HMM-based speech synthesis using average voice models trained on multiple speakers and speaker adaptation technologies. The file was previously available on the CSTR website, and was referenced in the Google DeepMind work on WaveNet: https://arxiv.org/pdf/1609.03499.pdf . Please note while text files containing transcripts of the speech are provided for 108 of the 109 recordings, in the '/txt' folder, the 'p315' text was lost due to a hard disk error.},
	language = {eng},
	urldate = {2021-10-18},
	journal = {The Rainbow Passage which the speakers read out can be found in the International Dialects of English Archive: (http://web.ku.edu/{\textasciitilde}idea/readings/rainbow.htm).},
	author = {Veaux, Christophe and Yamagishi, Junichi and MacDonald, Kirsten},
	month = apr,
	year = {2017},
}

@incollection{pernul_all_2015,
	address = {Cham},
	title = {All {Your} {Voices} are {Belong} to {Us}: {Stealing} {Voices} to {Fool} {Humans} and {Machines}},
	volume = {9327},
	isbn = {9783319241760 9783319241777},
	shorttitle = {All {Your} {Voices} are {Belong} to {Us}},
	url = {http://link.springer.com/10.1007/978-3-319-24177-7_30},
	language = {en},
	urldate = {2021-11-08},
	booktitle = {Computer {Security} -- {ESORICS} 2015},
	publisher = {Springer International Publishing},
	author = {Mukhopadhyay, Dibya and Shirvanian, Maliheh and Saxena, Nitesh},
	editor = {Pernul, Günther and Y A Ryan, Peter and Weippl, Edgar},
	year = {2015},
	doi = {10.1007/978-3-319-24177-7_30},
	pages = {599--621},
}

@inproceedings{neupane_crux_2019,
	address = {San Diego, CA},
	title = {The {Crux} of {Voice} ({In}){Security}: {A} {Brain} {Study} of {Speaker} {Legitimacy} {Detection}},
	isbn = {9781891562556},
	shorttitle = {The {Crux} of {Voice} ({In}){Security}},
	url = {https://www.ndss-symposium.org/wp-content/uploads/2019/02/ndss2019_08-3_Neupane_paper.pdf},
	doi = {10.14722/ndss.2019.23206},
	language = {en},
	urldate = {2021-11-08},
	booktitle = {Proceedings 2019 {Network} and {Distributed} {System} {Security} {Symposium}},
	publisher = {Internet Society},
	author = {Neupane, Ajaya and Saxena, Nitesh and Hirshfield, Leanne and Bratt, Sarah Elaine},
	year = {2019},
}

@inproceedings{ergunay_vulnerability_2015,
	title = {On the vulnerability of speaker verification to realistic voice spoofing},
	doi = {10.1109/BTAS.2015.7358783},
	abstract = {Automatic speaker verification (ASV) systems are subject to various kinds of malicious attacks. Replay, voice conversion and speech synthesis attacks drastically degrade the performance of a standard ASV system by increasing its false acceptance rates. This issue raised a high level of interest in the speech research community where the possible voice spoofing attacks and their related countermeasures have been investigated. However, much less effort has been devoted in creating realistic and diverse spoofing attack databases that foster researchers to correctly evaluate their countermeasures against attacks. The existing studies are not complete in terms of types of attacks, and often difficult to reproduce because of unavailability of public databases. In this paper we introduce the voice spoofing data-set of AVspoof, a public audio-visual spoofing database. AVspoof includes ten realistic spoofing threats generated using replay, speech synthesis and voice conversion. In addition, we provide a set of experimental results that show the effect of such attacks on current state-of-the-art ASV systems.},
	booktitle = {2015 {IEEE} 7th {International} {Conference} on {Biometrics} {Theory}, {Applications} and {Systems} ({BTAS})},
	author = {Ergünay, Serife Kucur and Khoury, Elie and Lazaridis, Alexandros and Marcel, Sébastien},
	month = sep,
	year = {2015},
	keywords = {Speech, Speech synthesis, Databases, Microphones, Hidden Markov models, Face, Biometrics (access control), spoofing, countermeasure, replay attack, speech synthesis, voice conversion, speaker verification},
	pages = {1--6},
}

@inproceedings{alegre_re-assessing_2014,
	title = {Re-assessing the threat of replay spoofing attacks against automatic speaker verification},
	abstract = {This paper re-examines the threat of spoofing or presentation attacks in the context of automatic speaker verification (ASV). While voice conversion and speech synthesis attacks present a serious threat, and have accordingly received a great deal of attention in the recent literature, they can only be implemented with a high level of technical know-how. In contrast, the implementation of replay attacks require no specific expertise nor any sophisticated equipment and thus they arguably present a greater risk. The comparative threat of each attack is re-examined in this paper against six different ASV systems including a state-of-the-art iVector-PLDA system. Despite the lack of attention in the literature, experiments show that low-effort replay attacks provoke higher levels of false acceptance than comparatively higher-effort spoofing attacks such as voice conversion and speech synthesis. Results therefore show the need to refocus research effort and to develop countermeasures against replay attacks in future work.},
	booktitle = {2014 {International} {Conference} of the {Biometrics} {Special} {Interest} {Group} ({BIOSIG})},
	author = {Alegre, Federico and Janicki, Artur and Evans, Nicholas},
	month = sep,
	year = {2014},
	keywords = {Speech synthesis, Speech, Hidden Markov models, Standards, Acoustics, Speaker recognition, Adaptation models},
	pages = {1--6},
}

@article{gao_detection_2020,
	title = {Detection and {Evaluation} of human and machine generated speech in spoofing attacks on automatic speaker verification systems},
	url = {http://arxiv.org/abs/2011.03689},
	abstract = {Automatic speaker verification (ASV) systems utilize the biometric information in human speech to verify the speaker's identity. The techniques used for performing speaker verification are often vulnerable to malicious attacks that attempt to induce the ASV system to return wrong results, allowing an impostor to bypass the system and gain access. Attackers use a multitude of spoofing techniques for this, such as voice conversion, audio replay, speech synthesis, etc. In recent years, easily available tools to generate deepfaked audio have increased the potential threat to ASV systems. In this paper, we compare the potential of human impersonation (voice disguise) based attacks with attacks based on machine-generated speech, on black-box and white-box ASV systems. We also study countermeasures by using features that capture the unique aspects of human speech production, under the hypothesis that machines cannot emulate many of the fine-level intricacies of the human speech production mechanism. We show that fundamental frequency sequence-related entropy, spectral envelope, and aperiodic parameters are promising candidates for robust detection of deepfaked speech generated by unknown methods.},
	urldate = {2021-11-12},
	journal = {arXiv:2011.03689 [cs, eess]},
	author = {Gao, Yang and Lian, Jiachen and Raj, Bhiksha and Singh, Rita},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.03689},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@article{gonzalez_hautamaki_acoustical_2017,
	title = {Acoustical and perceptual study of voice disguise by age modification in speaker verification},
	volume = {95},
	issn = {01676393},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639317300924},
	doi = {10.1016/j.specom.2017.10.002},
	language = {en},
	urldate = {2021-11-12},
	journal = {Speech Communication},
	author = {González Hautamäki, Rosa and Sahidullah, Md and Hautamäki, Ville and Kinnunen, Tomi},
	month = dec,
	year = {2017},
	pages = {1--15},
}

@inproceedings{shirvanian_quantifying_2019,
	title = {Quantifying the {Breakability} of {Voice} {Assistants}},
	doi = {10.1109/PERCOM.2019.8767399},
	abstract = {In this paper, we present a thorough study of voice impersonation attacks that can compromise the security of voice authentication technology deployed in several popular, state-of-the-art Android and iOS apps. Our study is based on our formulated Sneakers attack system that comprises a variety of well-known as well as newly designed attacks: (1) recorded and replayed voice of the authorized user (replay attack); (2) reordered and played-back voice of the authorized user (reorder attack); and (3) synthesized voice generated - based on voice conversion techniques - using an unauthorized user's voice (standard conversion attack), or using a noise-free recording from a text-to-speech engine (TTS conversion attack). Taking Sneakers as a basis, we report on a carefully designed study to examine a variety of real-world voice authentication apps for their vulnerability against malicious authentication. Our study follows a two-phase methodology. In the preliminary phase, we analyze 8 popular mobile apps against standard simplistic attack setups. Our results show that, while the tested apps seem to resist the reorder attack and the standard conversion attack, they are highly vulnerable to the replay attack. In the main phase of the study, we comprehensively assess 5 of the above apps against more advanced newly designed attack setups. Like in the preliminary phase, the apps prove to be highly vulnerable to the replay attack. More seriously, the apps also turn out to be highly insecure against our advanced attack setups, i.e., the reorder attack with coordinated timing and the TTS conversion attack, yielding success rates of 82\%-98\%. These malicious authentication measurement results are highly pertinent in practice because, we demonstrate that the apps generally work well in the benign authentication scenario to reliably "accept" an authorized user and "reject" an unauthorized user. Our work shows that many standard attacks that prior work demonstrated to be effective against standalone voice authentication algorithms do not work against current voice authentication apps. Yet, our new attack designs could still compromise these apps. Overall, our work highlights a serious vulnerability of real-world voice authentication apps, which seems very challenging to mitigate at a fundamental level.},
	booktitle = {2019 {IEEE} {International} {Conference} on {Pervasive} {Computing} and {Communications} ({PerCom}},
	author = {Shirvanian, Maliheh and Vo, Summer and Saxena, Nitesh},
	month = mar,
	year = {2019},
	note = {ISSN: 2474-249X},
	keywords = {Authentication, Standards, Training, Feature extraction, Smart phones, Conferences, Voice Authentication, Speaker Verification, Voice Synthesis Attack},
	pages = {1--11},
}

@misc{noauthor__nodate-1,
	title = { {Festvox}: {Home}. {Retrieved} {November} 12, 2021 from http://www.festvox.org/},
	url = {http://www.festvox.org/},
	urldate = {2021-11-12},
}

@article{belin_understanding_2011,
	title = {Understanding {Voice} {Perception}: {Understanding} voice perception},
	volume = {102},
	issn = {00071269},
	shorttitle = {Understanding {Voice} {Perception}},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.2044-8295.2011.02041.x},
	doi = {10.1111/j.2044-8295.2011.02041.x},
	language = {en},
	number = {4},
	urldate = {2021-11-22},
	journal = {British Journal of Psychology},
	author = {Belin, Pascal and Bestelmeyer, Patricia E. G. and Latinus, Marianne and Watson, Rebecca},
	month = nov,
	year = {2011},
	pages = {711--725},
}

@inproceedings{khoury_spear:_2014,
	address = {Florence, Italy},
	title = {Spear: {An} open source toolbox for speaker recognition based on {Bob}},
	isbn = {9781479928934},
	shorttitle = {Spear},
	url = {http://ieeexplore.ieee.org/document/6853879/},
	doi = {10.1109/ICASSP.2014.6853879},
	urldate = {2021-11-22},
	booktitle = {2014 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Khoury, Elie and Shafey, Laurent El and Marcel, Sebastien},
	month = may,
	year = {2014},
	pages = {1655--1659},
}

@inproceedings{anjos_bob:_2012,
	address = {Nara, Japan},
	title = {Bob: a free signal processing and machine learning toolbox for researchers},
	isbn = {9781450310895},
	shorttitle = {Bob},
	url = {http://dl.acm.org/citation.cfm?doid=2393347.2396517},
	doi = {10.1145/2393347.2396517},
	language = {en},
	urldate = {2021-11-22},
	booktitle = {Proceedings of the 20th {ACM} international conference on {Multimedia} - {MM} '12},
	publisher = {ACM Press},
	author = {Anjos, André and El-Shafey, Laurent and Wallace, Roy and Günther, Manuel and McCool, Christopher and Marcel, Sébastien},
	year = {2012},
	pages = {1449},
}

@article{reynolds_speaker_2000,
	title = {Speaker {Verification} {Using} {Adapted} {Gaussian} {Mixture} {Models}},
	volume = {10},
	issn = {10512004},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1051200499903615},
	doi = {10.1006/dspr.1999.0361},
	language = {en},
	number = {1-3},
	urldate = {2021-11-22},
	journal = {Digital Signal Processing},
	author = {Reynolds, Douglas A. and Quatieri, Thomas F. and Dunn, Robert B.},
	month = jan,
	year = {2000},
	pages = {19--41},
}

@article{vogt_explicit_2008,
	title = {Explicit modelling of session variability for speaker verification},
	volume = {22},
	issn = {08852308},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0885230807000277},
	doi = {10.1016/j.csl.2007.05.003},
	language = {en},
	number = {1},
	urldate = {2021-11-22},
	journal = {Computer Speech \& Language},
	author = {Vogt, Robbie and Sridharan, Sridha},
	month = jan,
	year = {2008},
	pages = {17--38},
}

@inproceedings{khoury_2013_2013,
	title = {The 2013 speaker recognition evaluation in mobile environment},
	doi = {10.1109/ICB.2013.6613025},
	abstract = {This paper evaluates the performance of the twelve primary systems submitted to the evaluation on speaker verification in the context of a mobile environment using the MOBIO database. The mobile environment provides a challenging and realistic test-bed for current state-of-the-art speaker verification techniques. Results in terms of equal error rate (EER), half total error rate (HTER) and detection error trade-off (DET) confirm that the best performing systems are based on total variability modeling, and are the fusion of several sub-systems. Nevertheless, the good old UBM-GMM based systems are still competitive. The results also show that the use of additional data for training as well as gender-dependent features can be helpful.},
	booktitle = {2013 {International} {Conference} on {Biometrics} ({ICB})},
	author = {Khoury, E. and Vesnicer, B. and Franco-Pedroso, J. and Violato, R. and Boulkcnafet, Z. and Mazaira Fernández, L. M. and Diez, M. and Kosmala, J. and Khemiri, H. and Cipr, T. and Saeidi, R. and Günther, M. and Žganec-Gros, J. and Candil, R. Zazo and Simões, F. and Bengherabi, M. and Álvarez Marquina, A. and Penagarikano, M. and Abad, A. and Boulayemen, M. and Schwarz, P. and Van Leeuwen, D. and González-Domínguez, J. and Neto, M. Uliani and Boutellaa, E. and Vilda, P. Gómez and Varona, A. and Petrovska-Delacrétaz, D. and Matějka, P. and González-Rodríguez, J. and Pereira, T. and Harizi, F. and Rodriguez-Fuentes, L. J. and Shafey, L. El and Angeloni, M. and Bordel, G. and Chollet, G. and Marcel, S.},
	month = jun,
	year = {2013},
	note = {ISSN: 2376-4201},
	keywords = {Feature extraction, Mel frequency cepstral coefficient, Databases, Training, NIST, Calibration, Logistics},
	pages = {1--8},
}

@inproceedings{huang_defending_2021,
	address = {Shenzhen, China},
	title = {Defending {Your} {Voice}: {Adversarial} {Attack} on {Voice} {Conversion}},
	isbn = {9781728170664},
	shorttitle = {Defending {Your} {Voice}},
	url = {https://ieeexplore.ieee.org/document/9383529/},
	doi = {10.1109/SLT48900.2021.9383529},
	urldate = {2021-11-22},
	booktitle = {2021 {IEEE} {Spoken} {Language} {Technology} {Workshop} ({SLT})},
	publisher = {IEEE},
	author = {Huang, Chien-yu and Lin, Yist Y. and Lee, Hung-yi and Lee, Lin-shan},
	month = jan,
	year = {2021},
	pages = {552--559},
}

@article{jain_biometric_2000,
	title = {Biometric identification},
	volume = {43},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/328236.328110},
	doi = {10.1145/328236.328110},
	language = {en},
	number = {2},
	urldate = {2021-11-22},
	journal = {Communications of the ACM},
	author = {Jain, Anil and Hong, Lin and Pankanti, Sharath},
	month = feb,
	year = {2000},
	pages = {90--98},
}

@misc{noauthor_resemblyzer._nodate,
	title = {Resemblyzer. {GitHub}. {Retrieved} {November} 22, 2021 from https://github.com/resemble-ai/{Resemblyzer}},
	url = {https://github.com/resemble-ai/Resemblyzer},
	abstract = {A python package to analyze and compare voices with deep learning},
	language = {en},
	urldate = {2021-11-22},
	journal = {GitHub},
}

@misc{noauthor_clone_nodate,
	title = {Clone {Synthetic} {AI} {Voices} with {Neural} {Text} to {Speech}. {Retrieved} {November} 22, 2021 from https://github.com/resemble-ai/{Resemblyzer}},
	url = {https://www.resemble.ai/},
	abstract = {Custom AI Voice Generator. Create realistic text-to-speech AI voices with Resemble's voice cloning software. Real-time API's and 44kHz audio.},
	language = {en-US},
	urldate = {2021-11-22},
	journal = {Resemble AI},
}

@incollection{ahmed_void:_2020,
	address = {USA},
	title = {Void: a fast and light voice liveness detection system},
	isbn = {9781939133175},
	shorttitle = {Void},
	abstract = {Due to the open nature of voice assistants' input channels, adversaries could easily record people's use of voice commands, and replay them to spoof voice assistants. To mitigate such spoofing attacks, we present a highly efficient voice liveness detection solution called "Void." Void detects voice spoofing attacks using the differences in spectral power between live-human voices and voices replayed through speakers. In contrast to existing approaches that use multiple deep learning models, and thousands of features, Void uses a single classification model with just 97 features. We used two datasets to evaluate its performance: (1) 255,173 voice samples generated with 120 participants, 15 playback devices and 12 recording devices, and (2) 18,030 publicly available voice samples generated with 42 participants, 26 playback devices and 25 recording devices. Void achieves equal error rate of 0.3\% and 11.6\% in detecting voice replay attacks for each dataset, respectively. Compared to a state of the art, deep learning-based solution that achieves 7.4\% error rate in that public dataset, Void uses 153 times less memory and is about 8 times faster in detection. When combined with a Gaussian Mixture Model that uses Melfrequency cepstral coefficients (MFCC) as classification features - MFCC is already being extracted and used as the main feature in speech recognition services - Void achieves 8.7\% error rate on the public dataset. Moreover, Void is resilient against hidden voice command, inaudible voice command, voice synthesis, equalization manipulation attacks, and combining replay attacks with live-human voices achieving about 99.7\%, 100\%, 90.2\%, 86.3\%, and 98.2\% detection rates for those attacks, respectively.},
	number = {151},
	urldate = {2021-11-22},
	booktitle = {Proceedings of the 29th {USENIX} {Conference} on {Security} {Symposium}},
	publisher = {USENIX Association},
	author = {Ahmed, Muhammad Ejaz and Kwak, Il-Youp and Huh, Jun Ho and Kim, Iljoo and Oh, Taekkyung and Kim, Hyoungshick},
	month = aug,
	year = {2020},
	pages = {2685--2702},
}

@article{jemine_master_2019,
	title = {Master thesis : {Real}-{Time} {Voice} {Cloning}},
	shorttitle = {Master thesis},
	url = {https://matheo.uliege.be/handle/2268.2/6801},
	abstract = {Recent advances in deep learning have shown impressive results in the domain of text-to-speech. To this end, a deep neural network is usually trained using a corpus of several hours of professionally recorded speech from a single speaker. Giving a new voice to such a model is highly expensive, as it requires recording a new dataset and retraining the model. A recent research introduced a three-stage pipeline that allows to clone a voice unseen during training from only a few seconds of reference speech, and without retraining the model. The authors share remarkably natural-sounding results, but provide no implementation. We reproduce this framework and open-source the first public implementation of it. We adapt the framework with a newer vocoder model, so as to make it run in real-time.},
	language = {en},
	urldate = {2021-11-24},
	author = {Jemine, Corentin and Info, Université de Liège {\textgreater} Bac sc},
	month = jun,
	year = {2019},
}

@book{beigi_fundamentals_2011,
	title = {Fundamentals of {Speaker} {Recognition}},
	isbn = {9780387775913},
	abstract = {An emerging technology, Speaker Recognition is becoming well-known for providing voice authentication over the telephone for helpdesks, call centres and other enterprise businesses for business process automation. "Fundamentals of Speaker Recognition" introduces Speaker Identification, Speaker Verification, Speaker (Audio Event) Classification, Speaker Detection, Speaker Tracking and more. The technical problems are rigorously defined, and a complete picture is made of the relevance of the discussed algorithms and their usage in building a comprehensive Speaker Recognition System. Designed as a textbook with examples and exercises at the end of each chapter, "Fundamentals of Speaker Recognition" is suitable for advanced-level students in computer science and engineering, concentrating on biometrics, speech recognition, pattern recognition, signal processing and, specifically, speaker recognition. It is also a valuable reference for developers of commercial technology and for speech scientists. Click here to view the table of contents and index.},
	publisher = {Springer Publishing Company, Incorporated},
	author = {Beigi, Homayoon},
	year = {2011},
}

@article{kameoka_stargan-vc:_2018,
	title = {{StarGAN}-{VC}: {Non}-parallel many-to-many voice conversion with star generative adversarial networks},
	shorttitle = {{StarGAN}-{VC}},
	url = {http://arxiv.org/abs/1806.02169},
	abstract = {This paper proposes a method that allows non-parallel many-to-many voice conversion (VC) by using a variant of a generative adversarial network (GAN) called StarGAN. Our method, which we call StarGAN-VC, is noteworthy in that it (1) requires no parallel utterances, transcriptions, or time alignment procedures for speech generator training, (2) simultaneously learns many-to-many mappings across different attribute domains using a single generator network, (3) is able to generate converted speech signals quickly enough to allow real-time implementations and (4) requires only several minutes of training examples to generate reasonably realistic-sounding speech. Subjective evaluation experiments on a non-parallel many-to-many speaker identity conversion task revealed that the proposed method obtained higher sound quality and speaker similarity than a state-of-the-art method based on variational autoencoding GANs.},
	urldate = {2021-11-29},
	journal = {arXiv:1806.02169 [cs, eess, stat]},
	author = {Kameoka, Hirokazu and Kaneko, Takuhiro and Tanaka, Kou and Hojo, Nobukatsu},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.02169},
	keywords = {Computer Science - Sound, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
}

@article{pasini_melgan-vc:_2019,
	title = {{MelGAN}-{VC}: {Voice} {Conversion} and {Audio} {Style} {Transfer} on arbitrarily long samples using {Spectrograms}},
	shorttitle = {{MelGAN}-{VC}},
	url = {http://arxiv.org/abs/1910.03713},
	abstract = {Traditional voice conversion methods rely on parallel recordings of multiple speakers pronouncing the same sentences. For real-world applications however, parallel data is rarely available. We propose MelGAN-VC, a voice conversion method that relies on non-parallel speech data and is able to convert audio signals of arbitrary length from a source voice to a target voice. We firstly compute spectrograms from waveform data and then perform a domain translation using a Generative Adversarial Network (GAN) architecture. An additional siamese network helps preserving speech information in the translation process, without sacrificing the ability to flexibly model the style of the target speaker. We test our framework with a dataset of clean speech recordings, as well as with a collection of noisy real-world speech examples. Finally, we apply the same method to perform music style transfer, translating arbitrarily long music samples from one genre to another, and showing that our framework is flexible and can be used for audio manipulation applications different from voice conversion.},
	urldate = {2021-11-29},
	journal = {arXiv:1910.03713 [cs, eess]},
	author = {Pasini, Marco},
	month = dec,
	year = {2019},
	note = {arXiv: 1910.03713},
	keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning, Computer Science - Sound},
}

@misc{amazon_what_nodate,
	type = {E-commerce},
	title = {What {Is} {Alexa} {Voice} {ID}?},
	url = {https://www.amazon.com/gp/help/customer/display.html?nodeId= GYCXKY2AB2QWZT2X.},
	language = {English},
	author = {Amazon},
	note = {publisher: Amazon},
}

@misc{noauthor_google_nodate,
	title = {Google {Assistant}, your own personal {Google}},
	url = {https://assistant.google.com/},
	abstract = {Meet your Google Assistant. Ask it questions. Tell it to do things. It's your own personal Google, always ready to help whenever you need it.},
	language = {en-US},
	urldate = {2021-11-29},
	journal = {Assistant},
}

@misc{the_official_wechat_blog_voiceprint:_2015,
	type = {Blog},
	title = {Voiceprint: {The} {New} {WeChat} {Password}},
	url = {https://blog.wechat.com/2015/05/21/voiceprint-the-new-wechat-password/},
	language = {En},
	author = {{The Official WeChat Blog}},
	month = may,
	year = {2015},
	note = {publisher: WeChat},
}

@misc{noauthor_voice_nodate,
	title = {Voice {ID} {\textbar} chase.com},
	url = {https://www.chase.com/personal/voice-biometrics},
	abstract = {Similar to a fingerprint, Voice ID uses your unique voiceprint to verify your identity.},
	urldate = {2021-11-29},
}

@misc{noauthor_voice_nodate-1,
	title = {Voice {ID} - {Customer} {Service} - {HSBC} {Bank} {USA}},
	url = {https://www.us.hsbc.com/customer-service/voice/},
	abstract = {Find out how to use your voice as your ID on HSBC US},
	language = {en-us},
	urldate = {2021-11-29},
}

@misc{noauthor_link_nodate,
	title = {Link your voice to your devices with {Voice} {Match} - {Android} - {Google} {Assistant} {Help}},
	url = {https://support.google.com/assistant/answer/9071681},
	abstract = {You can teach Google Assistant to recognize your voice with Voice Match. Then you can link your voice to a speaker, Smart Display, or Smart Clock so you can use voice commands to get personal results.},
	urldate = {2021-11-29},
}

@article{mirsky_creation_2021,
	title = {The {Creation} and {Detection} of {Deepfakes}: {A} {Survey}},
	volume = {54},
	issn = {0360-0300, 1557-7341},
	shorttitle = {The {Creation} and {Detection} of {Deepfakes}},
	url = {https://dl.acm.org/doi/10.1145/3425780},
	doi = {10.1145/3425780},
	abstract = {Generative deep learning algorithms have progressed to a point where it is difficult to tell the difference between what is real and what is fake. In 2018, it was discovered how easy it is to use this technology for unethical and malicious applications, such as the spread of misinformation, impersonation of political leaders, and the defamation of innocent individuals. Since then, these “deepfakes” have advanced significantly.
            In this article, we explore the creation and detection of deepfakes and provide an in-depth view as to how these architectures work. The purpose of this survey is to provide the reader with a deeper understanding of (1) how deepfakes are created and detected, (2) the current trends and advancements in this domain, (3) the shortcomings of the current defense solutions, and (4) the areas that require further research and attention.},
	language = {en},
	number = {1},
	urldate = {2021-12-01},
	journal = {ACM Computing Surveys},
	author = {Mirsky, Yisroel and Lee, Wenke},
	month = apr,
	year = {2021},
	pages = {1--41},
}


@incollection{juang_digital_2003,
	title = {Digital {Speech} {Processing}},
	isbn = {9780122274107},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B0122274105001782},
	language = {en},
	urldate = {2022-04-02},
	booktitle = {Encyclopedia of {Physical} {Science} and {Technology}},
	publisher = {Elsevier},
	author = {Juang, Biing Hwang and Sondhi, M.Mohan and Rabiner, Lawrence R.},
	year = {2003},
	doi = {10.1016/B0-12-227410-5/00178-2},
	pages = {485--500},
}

@incollection{boulmaiz_use_2020,
	title = {The use of {WSN} (wireless sensor network) in the surveillance of endangered bird species},
	isbn = {9780128168011},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780128168011000098},
	language = {en},
	urldate = {2022-04-02},
	booktitle = {Advances in {Ubiquitous} {Computing}},
	publisher = {Elsevier},
	author = {Boulmaiz, Amira and Doghmane, Noureddine and Harize, Saliha and Kouadria, Nasreddine and Messadeg, Djemil},
	year = {2020},
	doi = {10.1016/B978-0-12-816801-1.00009-8},
	pages = {261--306},
}

@misc{steven_h._weinberger_speech_2021,
	title = {Speech {Accent} {Archive}},
	url = {https://accent.gmu.edu/index.php},
	urldate = {2022-04-02},
	author = {{Steven H. Weinberger}},
	month = nov,
	year = {2021},
}

@inproceedings{panayotov_librispeech:_2015,
	address = {South Brisbane, Queensland, Australia},
	title = {Librispeech: {An} {ASR} corpus based on public domain audio books},
	isbn = {9781467369978},
	shorttitle = {Librispeech},
	url = {http://ieeexplore.ieee.org/document/7178964/},
	doi = {10.1109/ICASSP.2015.7178964},
	urldate = {2022-04-02},
	booktitle = {2015 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
	month = apr,
	year = {2015},
	pages = {5206--5210},
}

@inproceedings{ahamad_accentdb:_2020,
	address = {Marseille, France},
	title = {{AccentDB}: {A} {Database} of {Non}-{Native} {English} {Accents} to {Assist} {Neural} {Speech} {Recognition}},
	isbn = {9791095546344},
	shorttitle = {{AccentDB}},
	url = {https://aclanthology.org/2020.lrec-1.659},
	abstract = {Modern Automatic Speech Recognition (ASR) technology has evolved to identify the speech spoken by native speakers of a language very well. However, identification of the speech spoken by non-native speakers continues to be a major challenge for it. In this work, we first spell out the key requirements for creating a well-curated database of speech samples in non-native accents for training and testing robust ASR systems. We then introduce AccentDB, one such database that contains samples of 4 Indian-English accents collected by us, and a compilation of samples from 4 native-English, and a metropolitan Indian-English accent. We also present an analysis on separability of the collected accent data. Further, we present several accent classification models and evaluate them thoroughly against human-labelled accent classes. We test the generalization of our classifier models in a variety of setups of seen and unseen data. Finally, we introduce accent neutralization of non-native accents to native accents using autoencoder models with task-specific architectures. Thus, our work aims to aid ASR systems at every stage of development with a database for training, classification models for feature augmentation, and neutralization systems for acoustic transformations of non-native accents of English.},
	language = {English},
	urldate = {2022-04-02},
	booktitle = {Proceedings of the 12th {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Ahamad, Afroz and Anand, Ankit and Bhargava, Pranesh},
	month = may,
	year = {2020},
	pages = {5351--5358},
}